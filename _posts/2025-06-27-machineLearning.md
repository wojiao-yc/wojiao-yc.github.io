---
date: 2025-06-27 18:00:00
layout: post
title: Machine Learning Summary
subtitle: Recording and Learning
description: Summarize and memorize the content of machine learning
image: ..\assets\img\posts\MachineLearning\machineLearning.png
optimized_image: ..\assets\img\posts\MachineLearning\machineLearning.png
category: study
tags:
  - machine learning
author: wojiao-yc
---

## ç›®å½•
### [ä»‹ç»](#ä»‹ç»)

### ç»å…¸ç®—æ³•
[çº¿æ€§å›å½’(linear regression)](#çº¿æ€§å›å½’) &nbsp;&nbsp;&nbsp; [æ”¯æŒå‘é‡æœº(Support Vector Machine)](#æ”¯æŒå‘é‡æœº) &nbsp;&nbsp;&nbsp; [k-è¿‘é‚»(K-Nearest Neighbors)](#k-è¿‘é‚») &nbsp;&nbsp;&nbsp; [é€»è¾‘å›å½’(Logistic Regression)](#é€»è¾‘å›å½’) &nbsp;&nbsp;&nbsp; [å†³ç­–æ ‘(Decision Tree)](#å†³ç­–æ ‘) &nbsp;&nbsp;&nbsp; [k-å¹³å‡(k-means)](#k-å¹³å‡) &nbsp;&nbsp;&nbsp; [éšæœºæ£®æ—(Random Forest)](#éšæœºæ£®æ—) &nbsp;&nbsp;&nbsp; [æœ´ç´ è´å¶æ–¯(Naive Bayes)](#æœ´ç´ è´å¶æ–¯)

### æŸå¤±å‡½æ•°

### æ¿€æ´»å‡½æ•°

### å…¶å®ƒ
[æœ€å°äºŒä¹˜æ³•(least sqaure method)](#æœ€å°äºŒä¹˜æ³•)

## ä»‹ç»
æœºå™¨å­¦ä¹ ç®—æ³•å¤§è‡´å¯ä»¥åˆ†ä¸ºä»¥ä¸‹ä¸‰ç±»ï¼š

**ç›‘ç£å­¦ä¹ ç®—æ³•ï¼ˆSupervised Algorithmsï¼‰**:åœ¨ç›‘ç£å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥ç”±è®­ç»ƒæ•°æ®é›†å­¦åˆ°æˆ–å»ºç«‹ä¸€ä¸ªæ¨¡å¼ï¼ˆå‡½æ•° / learning modelï¼‰ï¼Œå¹¶ä¾æ­¤æ¨¡å¼æ¨æµ‹æ–°çš„å®ä¾‹ã€‚é€šè¿‡å·²æ ‡æ³¨çš„æ•°æ®ï¼ˆè¾“å…¥-è¾“å‡ºå¯¹ï¼‰å­¦ä¹ æ˜ å°„å…³ç³»ï¼Œç”¨äºé¢„æµ‹æˆ–åˆ†ç±»ã€‚å¦‚ï¼Œçº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœºã€‚

**æ— ç›‘ç£å­¦ä¹ ç®—æ³• (Unsupervised Algorithms)**:è¿™ç±»ç®—æ³•ä»æ— æ ‡æ³¨æ•°æ®ä¸­å‘ç°éšè—æ¨¡å¼æˆ–ç»“æ„ï¼Œæ²¡æœ‰ç‰¹å®šçš„ç›®æ ‡è¾“å‡ºï¼Œä¸€èˆ¬å°†æ•°æ®é›†åˆ†ä¸ºä¸åŒçš„ç»„ã€‚å¦‚ï¼ŒKå‡å€¼èšç±»ã€å±‚æ¬¡èšç±»ã€ä¸»æˆåˆ†åˆ†æã€è‡ªç¼–ç å™¨ã€‚

**å¼ºåŒ–å­¦ä¹ ç®—æ³• (Reinforcement Algorithms)**:é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œä»¥æœ€å¤§åŒ–é•¿æœŸå¥–åŠ±ã€‚ç®—æ³•æ ¹æ®è¾“å‡ºç»“æœï¼ˆå†³ç­–ï¼‰çš„æˆåŠŸæˆ–é”™è¯¯æ¥è®­ç»ƒè‡ªå·±ï¼Œé€šè¿‡å¤§é‡ç»éªŒè®­ç»ƒä¼˜åŒ–åçš„ç®—æ³•å°†èƒ½å¤Ÿç»™å‡ºè¾ƒå¥½çš„é¢„æµ‹ã€‚ç±»ä¼¼æœ‰æœºä½“åœ¨ç¯å¢ƒç»™äºˆçš„å¥–åŠ±æˆ–æƒ©ç½šçš„åˆºæ¿€ä¸‹ï¼Œé€æ­¥å½¢æˆå¯¹åˆºæ¿€çš„é¢„æœŸï¼Œäº§ç”Ÿèƒ½è·å¾—æœ€å¤§åˆ©ç›Šçš„ä¹ æƒ¯æ€§è¡Œä¸ºã€‚å¦‚ï¼ŒQ-Learningã€æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰ã€ç­–ç•¥æ¢¯åº¦ï¼ˆPolicy Gradientï¼‰ã€‚

---

## çº¿æ€§å›å½’

### ä¸€ã€å®šä¹‰ä¸æ ¸å¿ƒæ€æƒ³
ä¸€ç§ç”¨äºå»ºæ¨¡è¿ç»­å˜é‡ä¹‹é—´å…³ç³»çš„**ç›‘ç£å­¦ä¹ **ç®—æ³•ã€‚å…¶æ ¸å¿ƒå‡è®¾æ˜¯ç›®æ ‡å˜é‡ä¸ç‰¹å¾ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»ï¼Œå¹¶é€šè¿‡æ‹Ÿåˆæœ€ä½³ç›´çº¿ï¼ˆæˆ–è¶…å¹³é¢ï¼‰è¿›è¡Œé¢„æµ‹ï¼Œä½¿å¾—é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®æœ€å°ã€‚å…¶åˆåˆ†ä¸ºä¸¤ç§ç±»å‹ï¼Œå³åªæœ‰ä¸€ä¸ªè‡ªå˜é‡çš„ç®€å•çº¿æ€§å›å½’(simple linear regression)ä¸è‡³å°‘ä¸¤ç»„ä»¥ä¸Šè‡ªå˜é‡çš„å¤šå˜é‡å›å½’(multiple regression)ã€‚

### äºŒã€ç›®æ ‡
æ‰¾åˆ°æœ€ä¼˜çš„æƒé‡ $w$ å’Œåç½® $b$ï¼Œä½¿å¾—é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æœ€å°åŒ–ã€‚

### ä¸‰ã€è¯¦ç»†å†…å®¹
æ¨èçœ‹è¿™ç¯‡[çŸ¥ä¹ä¸“æ ](https://zhuanlan.zhihu.com/p/72513104)ã€‚

### å››ã€å®ç°æµç¨‹
1. åˆå§‹åŒ–æ¨¡å‹å‚æ•°ï¼ˆæƒé‡wå’Œåç½®bï¼‰ï¼Œé€šå¸¸è®¾ä¸º0æˆ–å°çš„éšæœºå€¼
2. è®¡ç®—é¢„æµ‹å€¼å¹¶è®¡ç®—æŸå¤±å‡½æ•°ï¼ˆé€šå¸¸ä½¿ç”¨å‡æ–¹è¯¯å·®MSEï¼‰
3. è®¡ç®—æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦
4. ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°å‚æ•°

### äº”ã€ä»£ç å®ç°
```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score

class LinearRegression:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        """
        Initialize Linear Regression model
        
        Parameters:
        learning_rate -- step size for parameter updates (default 0.01)
        n_iterations -- number of training iterations (default 1000)
        """
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None  # weight parameters
        self.bias = None     # bias parameter
        self.loss_history = []  # to record loss at each iteration
    
    def fit(self, X, y):
        """
        Train the linear regression model
        
        Parameters:
        X -- feature matrix of shape (n_samples, n_features)
        y -- target vector of shape (n_samples,)
        """
        n_samples, n_features = X.shape
        
        # 1. Initialize parameters
        self.weights = np.zeros(n_features)  # initialize weights to 0
        self.bias = 0                       # initialize bias to 0
        
        # 2. Gradient descent iterations
        for _ in range(self.n_iterations):
            # Forward pass: compute predictions
            y_pred = np.dot(X, self.weights) + self.bias
            
            # Compute loss (mean squared error)
            loss = (1 / (2 * n_samples)) * np.sum((y_pred - y) ** 2)
            self.loss_history.append(loss)
            
            # Compute gradients
            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))  # weight gradients
            db = (1 / n_samples) * np.sum(y_pred - y)         # bias gradient
            
            # Update parameters
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
    
    def predict(self, X):
        """
        Make predictions using the trained model
        
        Parameters:
        X -- feature matrix of shape (n_samples, n_features)
        
        Returns:
        y_pred -- predicted values of shape (n_samples,)
        """
        return np.dot(X, self.weights) + self.bias
```


### å…­ã€é¢ç»
**Q**ï¼šçº¿æ€§å›å½’çš„åŸºæœ¬å‡è®¾æœ‰å“ªäº›ï¼Ÿ
> çº¿æ€§å…³ç³»å‡è®¾ï¼ˆè‡ªå˜é‡Xä¸å› å˜é‡yä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»ï¼‰ï¼Œè¯¯å·®é¡¹ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆæ— è‡ªç›¸å…³ï¼‰ï¼Œè¯¯å·®é¡¹ä¹‹é—´æ— ç›¸å…³æ€§ï¼ˆå°¤å…¶æ—¶é—´åºåˆ—æ•°æ®ï¼‰ï¼Œè¯¯å·®é¡¹æ­£æ€åˆ†å¸ƒï¼ˆæ®‹å·®åº”æœä»å‡å€¼ä¸º0çš„æ­£æ€åˆ†å¸ƒï¼‰

**Q**ï¼šå¦‚ä½•åˆ¤æ–­çº¿æ€§å›å½’æ¨¡å‹çš„å¥½åï¼Ÿ
> RÂ²åˆ†æ•°æ¥è§£é‡Šæ¨¡å‹çš„æ–¹å·®è§£é‡Šèƒ½åŠ›(0-1ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½)ï¼Œå‡æ–¹è¯¯å·®(MSE)/å‡æ–¹æ ¹è¯¯å·®(RMSE)è¶Šå°è¶Šå¥½ï¼Œæ®‹å·®åˆ†æï¼ˆæ£€æŸ¥æ®‹å·®æ˜¯å¦éšæœºåˆ†å¸ƒï¼‰ï¼Œä¹Ÿå¯ä»¥å¯¹æ¯”è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¡¨ç°æ¥åˆ¤æ–­è¿‡æ‹Ÿåˆã€‚

**Q**ï¼šä»€ä¹ˆæ˜¯RÂ²åˆ†æ•°ï¼Ÿ
> æ˜¯è¯„ä¼°çº¿æ€§å›å½’æ¨¡å‹æ‹Ÿåˆä¼˜åº¦çš„æŒ‡æ ‡ï¼Œè¡¨ç¤ºæ¨¡å‹èƒ½å¤Ÿè§£é‡Šçš„ç›®æ ‡å˜é‡æ–¹å·®æ¯”ä¾‹ã€‚å…¶å–å€¼èŒƒå›´é€šå¸¸åœ¨0åˆ°1ä¹‹é—´ï¼Œæ•°å€¼è¶Šå¤§è¡¨ç¤ºæ¨¡å‹è§£é‡Šèƒ½åŠ›è¶Šå¼ºã€‚RÂ²é€šè¿‡æ¯”è¾ƒæ¨¡å‹é¢„æµ‹è¯¯å·®ï¼ˆå®é™…å€¼ä¸æ¨¡å‹é¢„æµ‹å€¼çš„å·®å¼‚å¹³æ–¹å’Œï¼‰å’ŒåŸºå‡†è¯¯å·®ï¼ˆå®é™…å€¼ä¸å‡å€¼çš„å·®å¼‚å¹³æ–¹å’Œï¼‰æ¥è®¡ç®—ï¼Œå…·ä½“æ¥è¯´è®¡ç®—å…¬å¼ä¸ºï¼šRÂ² = 1 - (æ¨¡å‹é¢„æµ‹è¯¯å·® / åŸºå‡†è¯¯å·®)

[å›åˆ°ç›®å½•](#ç›®å½•)

---

## æ”¯æŒå‘é‡æœº

### ä¸€ã€å®šä¹‰ä¸æ ¸å¿ƒæ€æƒ³
æ”¯æŒå‘é‡æœºæ˜¯ä¸€ç§å¼ºå¤§çš„**ç›‘ç£å­¦ä¹ **ç®—æ³•ï¼Œä¸»è¦ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œä¹Ÿå¯ç”¨äºå›å½’ï¼ˆç§°ä¸ºæ”¯æŒå‘é‡å›å½’ï¼ŒSVRï¼‰ã€‚å¯¹äºçº¿æ€§å¯åˆ†çš„æ•°æ®é›†æ¥è¯´ï¼Œè¿™æ ·çš„è¶…å¹³é¢æœ‰æ— ç©·å¤šä¸ªï¼ˆå³æ„ŸçŸ¥æœºï¼‰ï¼Œä½†æ˜¯å‡ ä½•é—´éš”æœ€å¤§çš„åˆ†ç¦»è¶…å¹³é¢å´æ˜¯å”¯ä¸€çš„ã€‚SVMè¿˜åŒ…æ‹¬æ ¸æŠ€å·§ï¼Œå³å¯¹äºè¾“å…¥ç©ºé—´ä¸­çš„éçº¿æ€§åˆ†ç±»é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡éçº¿æ€§å˜æ¢å°†å®ƒè½¬åŒ–ä¸ºæŸä¸ªç»´ç‰¹å¾ç©ºé—´ä¸­çš„çº¿æ€§åˆ†ç±»é—®é¢˜ï¼Œåœ¨é«˜ç»´ç‰¹å¾ç©ºé—´ä¸­å­¦ä¹ çº¿æ€§æ”¯æŒå‘é‡æœºï¼Œè¿™ä½¿å®ƒæˆä¸ºå®è´¨ä¸Šçš„éçº¿æ€§åˆ†ç±»å™¨ã€‚SVMçš„çš„å­¦ä¹ ç­–ç•¥å°±æ˜¯é—´éš”æœ€å¤§åŒ–ï¼Œå¯å½¢å¼åŒ–ä¸ºä¸€ä¸ªæ±‚è§£å‡¸äºŒæ¬¡è§„åˆ’çš„é—®é¢˜ã€‚æ€»çš„æ¥è¯´ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯å¯»æ‰¾ä¸€ä¸ªæœ€ä¼˜è¶…å¹³é¢ï¼Œæœ€å¤§åŒ–ä¸åŒç±»åˆ«æ•°æ®ä¹‹é—´çš„è¾¹ç•Œï¼ˆé—´éš”ï¼‰ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

### äºŒã€ç›®æ ‡
- æœ€å¤§åŒ–åˆ†ç±»é—´éš”ï¼Œå³æ‰¾åˆ°ä½¿é—´éš”æœ€å¤§çš„è¶…å¹³é¢ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
- æœ€å°åŒ–åˆ†ç±»é”™è¯¯ï¼ˆåœ¨è½¯é—´éš” SVM ä¸­ï¼Œå…è®¸å°‘é‡æ ·æœ¬è¿åé—´éš”çº¦æŸï¼‰ã€‚

### ä¸‰ã€è¯¦ç»†å†…å®¹
è¯¥[çŸ¥ä¹ä¸“æ ](https://www.zhihu.com/tardis/zm/art/31886934?source_id=1005)æä¾›äº†æä¸ºè¯¦ç»†çš„æ•°å­¦æ¨å¯¼ï¼Œè¿™ä¸ª[çŸ¥ä¹ä¸“æ ](https://zhuanlan.zhihu.com/p/77750026)åˆ™æ›´ä¸ºé€šä¿—æ˜“æ‡‚ã€‚

### å››ã€å®ç°æµç¨‹
1. æ„é€ æœ€ä¼˜åŒ–é—®é¢˜ï¼Œæ±‚è§£å‡ºæœ€ä¼˜åŒ–çš„æ‰€æœ‰Î±
2. è®¡ç®—å‚æ•° $w$ å’Œ $b$
3. å¾—å‡ºè¶…å¹³é¢ä¸å†³ç­–å‡½æ•°

### äº”ã€ä»£ç å®ç°
```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®é›†
iris = datasets.load_iris()
X = iris.data
y = iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# åˆ›å»ºSVMåˆ†ç±»å™¨
clf = SVC(kernel='linear')

# è®­ç»ƒæ¨¡å‹
clf.fit(X_train, y_train)

# é¢„æµ‹
y_pred = clf.predict(X_test)

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
```
ç›´æ¥ä½¿ç”¨è§[è¿™ç¯‡](https://geek-blogs.com/blog/svm-in-python/)ï¼Œæ‰‹åŠ¨å®ç°å¯ä»¥çœ‹[è¿™ç¯‡](https://blog.csdn.net/m0_56694518/article/details/134957004)ã€‚

### å…­ã€é¢ç»
**Q**ï¼šSVMä¸ºä»€ä¹ˆè¿½æ±‚"æœ€å¤§é—´éš”"ï¼Ÿ
> æœ€å¤§é—´éš”èƒ½æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å†³ç­–è¾¹ç•Œå¯¹å™ªå£°å’Œæ•°æ®æ‰°åŠ¨æ›´é²æ£’ã€‚é—´éš”è¶Šå¤§ï¼Œæœªæ¥æ–°æ•°æ®è¢«åˆ†ç±»é”™è¯¯çš„å¯èƒ½æ€§è¶Šå°ã€‚

**Q**ï¼šSVMçš„ä¼˜ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
>ï¼ˆ1ï¼‰**åœ¨é«˜ç»´ç©ºé—´ä¸­è¡¨ç°ä¼˜ç§€**ï¼šè®­ç»ƒå¥½çš„æ¨¡å‹çš„ç®—æ³•å¤æ‚åº¦æ˜¯ç”±æ”¯æŒå‘é‡çš„ä¸ªæ•°å†³å®šçš„ï¼Œè€Œä¸æ˜¯ç”±æ•°æ®çš„ç»´åº¦å†³å®šçš„ï¼Œæ‰€ä»¥SVMä¹Ÿä¸å¤ªå®¹æ˜“äº§ç”Ÿoverfitting
 ï¼ˆ2ï¼‰**æ ¸æŠ€å·§çµæ´»**ï¼Œèƒ½é€‚åº”å¤æ‚éçº¿æ€§é—®é¢˜ï¼šé€šè¿‡æ ¸å‡½æ•°ï¼ŒSVMå¯ä»¥éšå¼åœ°å°†æ•°æ®æ˜ å°„åˆ°æ›´é«˜ç»´ç©ºé—´ï¼Œä»è€Œè§£å†³çº¿æ€§ä¸å¯åˆ†é—®é¢˜
 ï¼ˆ3ï¼‰ä¾èµ–æ”¯æŒå‘é‡è€Œéå…¨éƒ¨æ•°æ®ï¼Œ**å†…å­˜æ•ˆç‡é«˜**ï¼šè®­ç»ƒå®Œæˆåï¼Œå¯ä»¥ä¸¢å¼ƒéæ”¯æŒå‘é‡çš„æ ·æœ¬ï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´ï¼Œé€‚åˆèµ„æºå—é™çš„åœºæ™¯

**Q**ï¼šSVMçš„ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
> ï¼ˆ1ï¼‰**å¯¹å‚æ•°å’Œæ ¸å‡½æ•°é€‰æ‹©æ•æ„Ÿ**ï¼šæƒ©ç½šç³»æ•° $C$ æ§åˆ¶æ¨¡å‹å¯¹åˆ†ç±»é”™è¯¯çš„å®¹å¿åº¦ï¼Œè¶Šå¤§è¯´æ˜è¶Šä¸èƒ½å®¹å¿å‡ºç°è¯¯å·®ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆï¼›è¶Šå°åˆ™è¶Šå®¹æ˜“æ¬ æ‹Ÿåˆã€‚å¦å¤–ä¸åŒçš„æ ¸å‡½æ•°å¯¹ç»“æœå½±å“å¾ˆå¤§ï¼Œéœ€è¦äº¤å‰éªŒè¯è°ƒæ•´
ï¼ˆ2ï¼‰**é»‘ç›’æ€§è¾ƒå¼º**ï¼Œå¯è§£é‡Šæ€§å·®ï¼šç›¸æ¯”é€»è¾‘å›å½’èƒ½åˆ†ææƒé‡å’Œå†³ç­–æ ‘èƒ½å¯è§†åŒ–è§„åˆ™ï¼ŒSVMçš„å†³ç­–è¿‡ç¨‹è¾ƒéš¾è§£é‡Šï¼Œä¸”æ— æ³•ç›´æ¥è¾“å‡ºæ¦‚ç‡ï¼ˆéœ€é¢å¤–æ ¡å‡†ï¼Œå¦‚Platt Scalingï¼‰ã€‚

**Q**ï¼šSVMä¸ºä»€ä¹ˆè¿½æ±‚"æœ€å¤§é—´éš”"ï¼Ÿ
> æœ€å¤§é—´éš”èƒ½æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å†³ç­–è¾¹ç•Œå¯¹å™ªå£°å’Œæ•°æ®æ‰°åŠ¨æ›´é²æ£’ã€‚é—´éš”è¶Šå¤§ï¼Œæœªæ¥æ–°æ•°æ®è¢«åˆ†ç±»é”™è¯¯çš„å¯èƒ½æ€§è¶Šå°ã€‚

**Q**ï¼šSVMå¦‚ä½•è¿›è¡Œæ¦‚ç‡é¢„æµ‹ï¼Ÿ
> å¯ä»¥ä½¿ç”¨ Platt Scaling è¿›è¡Œæ ¡å‡†ï¼Œåœ¨SVMçš„è¾“å‡ºï¼ˆå†³ç­–å‡½æ•°å€¼ï¼‰ä¸Šè®­ç»ƒä¸€ä¸ªé€»è¾‘å›å½’æ¨¡å‹ï¼Œå°†å…¶æ˜ å°„åˆ° [0,1] åŒºé—´ã€‚

[å›åˆ°ç›®å½•](#ç›®å½•)

---

## k-è¿‘é‚»

### å®šä¹‰ä¸æ ¸å¿ƒæ€æƒ³
K-è¿‘é‚» æ˜¯ä¸€ç§åŸºäºæ ·æœ¬çš„**ç›‘ç£å­¦ä¹ **ç®—æ³•ï¼Œå¯ç”¨äºåˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šç›¸ä¼¼çš„æ•°æ®ç‚¹åœ¨ç‰¹å¾ç©ºé—´ä¸­è·ç¦»è¾ƒè¿‘ï¼Œå› æ­¤æ–°æ ·æœ¬çš„ç±»åˆ«æˆ–å€¼å¯ä»¥ç”±å…¶æœ€è¿‘çš„Kä¸ªé‚»å±…å†³å®šã€‚å…·ä½“æ¥è¯´æ˜¯ç»™å®šä¸€ä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œå¯¹æ–°çš„è¾“å…¥æ ·æœ¬ï¼Œåœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ‰¾åˆ°ä¸è¯¥å®ä¾‹æœ€é‚»è¿‘çš„Kä¸ªæ ·æœ¬ï¼Œè¿™Kä¸ªæ ·æœ¬çš„å¤šæ•°å±äºæŸä¸ªç±»ï¼Œå°±æŠŠè¯¥è¾“å…¥æ ·æœ¬å‘¢åˆ†ç±»åˆ°è¿™ä¸ªç±»ä¸­ã€‚ï¼ˆå°‘æ•°æœä»å¤šæ•°ï¼‰

### ç›®æ ‡
- åˆ†ç±»ä»»åŠ¡ï¼šåŸºäºKä¸ªæœ€è¿‘é‚»çš„å¤šæ•°æŠ•ç¥¨ï¼Œé¢„æµ‹æ–°æ ·æœ¬çš„ç±»åˆ«ã€‚
- å›å½’ä»»åŠ¡ï¼šåŸºäºKä¸ªæœ€è¿‘é‚»çš„å¹³å‡å€¼ï¼Œé¢„æµ‹æ–°æ ·æœ¬çš„è¿ç»­å€¼ã€‚

### è¯¦ç»†å†…å®¹
æ¨èçœ‹è¿™ç¯‡[æ–‡ç« ](https://blog.csdn.net/m0_74405427/article/details/133714384)ã€‚

### å®ç°æµç¨‹
1. è·ç¦»è®¡ç®—ï¼šå¯¹äºå¾…åˆ†ç±»çš„æ ·æœ¬ç‚¹ï¼Œè®¡ç®—å®ƒä¸è®­ç»ƒé›†ä¸­æ¯ä¸ªæ ·æœ¬ç‚¹çš„è·ç¦»
2. é€‰æ‹©æœ€è¿‘é‚»ï¼šæ ¹æ®è®¡ç®—çš„è·ç¦»ï¼Œé€‰æ‹©è·ç¦»æœ€è¿‘çš„kä¸ªè®­ç»ƒæ ·æœ¬ï¼Œå³k-è¿‘é‚»åå­—çš„ç”±æ¥
3. æŠ•ç¥¨å†³ç­–ï¼šå¯¹äºåˆ†ç±»é—®é¢˜ç»Ÿè®¡kä¸ªæœ€è¿‘é‚»ä¸­å„ç±»åˆ«çš„æ•°é‡ï¼Œå°†å¾…åˆ†ç±»æ ·æœ¬å½’ä¸ºæ•°é‡æœ€å¤šçš„ç±»åˆ«ï¼›å¯¹äºå›å½’é—®é¢˜ï¼šå–kä¸ªæœ€è¿‘é‚»çš„ç›®æ ‡å€¼çš„å¹³å‡å€¼ä½œä¸ºé¢„æµ‹å€¼

### ä»£ç å®ç°
```python
import numpy as np
from collections import Counter
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

class KNN:
    """
    K-Nearest Neighbors classifier manual implementation
    
    Parameters:
        k: int, optional (default=5), number of nearest neighbors to consider
    """
    
    def __init__(self, k=5):
        self.k = k
    
    def fit(self, X, y):
        """
        Train the KNN model (actually just stores the data as KNN is a lazy learner)
        
        Parameters:
            X: Training feature data, array of shape [n_samples, n_features]
            y: Training target values, array of shape [n_samples]
        """
        # Standardize data (improves KNN performance)
        self.scaler = StandardScaler()
        self.X_train = self.scaler.fit_transform(X)
        self.y_train = y
        
    def predict(self, X):
        """
        Make predictions for test data
        
        Parameters:
            X: Test feature data, array of shape [n_samples, n_features]
            
        Returns:
            Predictions, array of shape [n_samples]
        """
        # Standardize test data (using mean and variance from training data)
        X = self.scaler.transform(X)
        # Initialize prediction array
        predictions = np.zeros(X.shape[0], dtype=self.y_train.dtype)
        
        # Make prediction for each test sample
        for i, x in enumerate(X):
            # 1. Calculate distances between current test sample and all training samples
            distances = self._compute_distances(x)
            
            # 2. Get indices of k nearest neighbors
            k_indices = np.argpartition(distances, self.k)[:self.k]
            
            # 3. Get labels of these k neighbors
            k_nearest_labels = self.y_train[k_indices]
            
            # 4. Vote for prediction (take most common class)
            most_common = Counter(k_nearest_labels).most_common(1)
            predictions[i] = most_common[0][0]
            
        return predictions
    
    def _compute_distances(self, x):
        """
        Calculate distances between one sample and all training samples (Euclidean distance)
        
        Parameters:
            x: Single sample point, array of shape [n_features]
            
        Returns:
            Array of distances, array of shape [n_train_samples]
        """
        # Euclidean distance calculation: sqrt(sum((x1 - x2)^2))
        distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))
        return distances
```
### é¢ç»
**Q**ï¼š å¦‚ä½•é€‰æ‹©åˆé€‚çš„Kå€¼ï¼Ÿ
>é€šè¿‡äº¤å‰éªŒè¯ï¼ˆä¸€ç§è¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„ç»Ÿè®¡æ–¹æ³•ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¤šæ¬¡åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œå‡å°‘æ¨¡å‹è¯„ä¼°çš„éšæœºæ€§ï¼Œä»è€Œæ›´å‡†ç¡®åœ°ä¼°è®¡æ¨¡å‹åœ¨æœªçŸ¥æ•°æ®ä¸Šçš„è¡¨ç°ï¼‰å°è¯•ä¸åŒKå€¼ï¼Œé€‰æ‹©åœ¨éªŒè¯é›†ä¸Šè¡¨ç°æœ€å¥½çš„K

**Q**ï¼šè®²ä¸€ä¸‹KNNçš„ä¼˜ç‚¹
>ï¼ˆ1ï¼‰**ç®€å•ç›´è§‚ï¼Œæ˜“äºå®ç°**:ç›´æ¥é€šè¿‡è·ç¦»è®¡ç®—æ‰¾åˆ°æœ€è¿‘çš„é‚»å±…ï¼Œæ— éœ€å¤æ‚çš„æ•°å­¦æ¨å¯¼
ï¼ˆ2ï¼‰**æ— éœ€è®­ç»ƒé˜¶æ®µ**:KNNä»…å­˜å‚¨è®­ç»ƒæ•°æ®ï¼Œé¢„æµ‹æ—¶æ‰è®¡ç®—è·ç¦»ï¼Œæ— éœ€æ˜¾å¼è®­ç»ƒå‚æ•°
ï¼ˆ3ï¼‰**å¯¹æ•°æ®åˆ†å¸ƒæ²¡æœ‰å‡è®¾**ï¼šä¸åƒçº¿æ€§å›å½’å‡è®¾æ•°æ®çº¿æ€§å¯åˆ†ï¼Œæˆ–é«˜æ–¯æœ´ç´ è´å¶æ–¯å‡è®¾ç‰¹å¾ç¬¦åˆæ­£æ€åˆ†å¸ƒã€‚KNNé€šè¿‡å±€éƒ¨é‚»å±…æŠ•ç¥¨ï¼Œèƒ½æ•æ‰éçº¿æ€§å…³ç³»

**Q**ï¼šè®²ä¸€ä¸‹KNNçš„ç¼ºç‚¹
>ï¼ˆ1ï¼‰è®¡ç®—å¤æ‚åº¦é«˜ï¼šéœ€ä¿å­˜å…¨éƒ¨è®­ç»ƒæ•°æ®ï¼Œå†…å­˜å ç”¨å¤§ï¼Œå¯¹æ–°æ ·æœ¬éœ€è®¡ç®—ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦»ï¼Œæ—¶é—´å¤æ‚åº¦ä¸ºO(NÂ·d)
ï¼ˆ2ï¼‰é«˜ç»´æ•°æ®æ•ˆæœå·®ï¼ˆç»´åº¦ç¾éš¾ï¼‰ï¼šåœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œæ‰€æœ‰æ ·æœ¬çš„è·ç¦»è¶‹äºç›¸ä¼¼ï¼ˆæ¬§æ°è·ç¦»åŒºåˆ†åº¦ä¸‹é™ï¼‰ï¼Œå¯¼è‡´é‚»å±…å¤±å»æ„ä¹‰
ï¼ˆ3ï¼‰å¯¹ä¸å¹³è¡¡æ•°æ®æ•æ„Ÿï¼šè‹¥æŸç±»æ ·æœ¬å 90%ï¼ŒKä¸ªé‚»å±…ä¸­å¤§æ¦‚ç‡å…¨æ˜¯å¤šæ•°ç±»ï¼Œå°‘æ•°ç±»æ˜“è¢«å¿½ç•¥
ï¼ˆ4ï¼‰éœ€è¦ç‰¹å¾ç¼©æ”¾ï¼šè‹¥ç‰¹å¾å°ºåº¦å·®å¼‚å¤§ï¼ˆå¦‚å¹´é¾„[0-100]å’Œå·¥èµ„[0-100000]ï¼‰ï¼Œå·¥èµ„ä¼šä¸»å¯¼è·ç¦»è®¡ç®—


[å›åˆ°ç›®å½•](#ç›®å½•)

---

## é€»è¾‘å›å½’

### ä¸€ã€å®šä¹‰ä¸æ ¸å¿ƒæ€æƒ³

é€»è¾‘å›å½’æ˜¯ä¸€ç§å…¸å‹çš„**ç›‘ç£å­¦ä¹ **ç®—æ³•ï¼Œæ¨¡å‹ä»å¸¦æœ‰æ ‡ç­¾çš„è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ è§„å¾‹ï¼Œå¹¶å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚è™½ç„¶è¢«ç§°ä¸ºå›å½’ï¼Œä½†å…¶å®é™…ä¸Šæ˜¯åˆ†ç±»æ¨¡å‹ï¼Œå¹¶å¸¸ç”¨äºäºŒåˆ†ç±»ã€‚æœ¬è´¨æ˜¯å‡è®¾æ•°æ®æœä»æŸä¸ªåˆ†å¸ƒï¼Œç„¶åä½¿ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡åšå‚æ•°çš„ä¼°è®¡ã€‚å…¶æ ¸å¿ƒæ€æƒ³åˆ™æ˜¯é€šè¿‡çº¿æ€§æ¨¡å‹é¢„æµ‹æ¦‚ç‡å¹¶ç”¨é€»è¾‘å‡½æ•°ï¼ˆSigmoidå‡½æ•°ï¼‰å°†çº¿æ€§ç»“æœæ˜ å°„åˆ°[0,1]åŒºé—´ã€‚

### äºŒã€ç›®æ ‡

é€»è¾‘å›å½’çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ç»„æ¨¡å‹å‚æ•°ï¼ˆæƒé‡$ğ‘¤$å’Œåç½®$ğ‘$ï¼‰ï¼Œä½¿å¾—æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡å°½å¯èƒ½æ¥è¿‘çœŸå®çš„ç±»åˆ«æ ‡ç­¾ã€‚è¿™ä¸€ç›®æ ‡é€šè¿‡æœ€å¤§åŒ–ä¼¼ç„¶å‡½æ•°ï¼ˆæˆ–æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼‰æ¥å®ç°ã€‚

### ä¸‰ã€è¯¦ç»†å†…å®¹
æ¨èçœ‹è¿™ç¯‡[æ–‡ç« ](https://zhuanlan.zhihu.com/p/139122386)ã€‚

### å››ã€å®ç°æµç¨‹
å°±æ˜¯ç”¨äº¤å‰ç†µï¼ˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡å’Œäº¤å‰ç†µåœ¨é€»è¾‘å›å½’ä¸­æ˜¯ç­‰ä»·çš„ï¼‰ä½œä¸ºæŸå¤±å‡½æ•°æ¢¯åº¦ä¸‹é™è®­ç»ƒä¸€ä¸ªæ„ŸçŸ¥æœºç„¶åæ±‚softmaxæ¦‚ç‡

### äº”ã€ä»£ç å®ç°
```python
import numpy as np
from sklearn.preprocessing import StandardScaler

class LogisticRegression:
    """
    A manually implemented Logistic Regression class
    """
    
    def __init__(self, learning_rate=0.01, n_iterations=1000, fit_intercept=True, verbose=False):
        """
        Initialize the Logistic Regression model
        
        Parameters:
        learning_rate -- Step size for gradient descent (default: 0.01)
        n_iterations -- Number of iterations for gradient descent (default: 1000)
        fit_intercept -- Whether to add an intercept term (default: True)
        verbose -- Whether to print training progress (default: False)
        """
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.fit_intercept = fit_intercept
        self.verbose = verbose
        self.weights = None
        self.scaler = StandardScaler()  # For feature standardization
    
    def __add_intercept(self, X):
        """
        Add intercept term (column of 1's) to feature matrix
        
        Parameters:
        X -- Input feature matrix (n_samples, n_features)
        
        Returns:
        Feature matrix with intercept term added (n_samples, n_features + 1)
        """
        intercept = np.ones((X.shape[0], 1))
        return np.concatenate((intercept, X), axis=1)
    
    def __sigmoid(self, z):
        """
        Sigmoid activation function mapping input to (0,1) interval
        
        Parameters:
        z -- Linear combination of inputs and weights
        
        Returns:
        Probability after sigmoid transformation
        """
        return 1 / (1 + np.exp(-z))
    
    def __loss(self, h, y):
        """
        Compute the logistic loss (log loss/cross-entropy loss)
        
        Parameters:
        h -- Predicted probabilities (n_samples,)
        y -- True labels (n_samples,)
        
        Returns:
        Current loss value
        """
        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()
    
    def fit(self, X, y):
        """
        Train the logistic regression model
        
        Parameters:
        X -- Training feature matrix (n_samples, n_features)
        y -- Training label vector (n_samples,)
        """
        # Standardize features
        X = self.scaler.fit_transform(X)
        
        # Add intercept term
        if self.fit_intercept:
            X = self.__add_intercept(X)
        
        # Initialize weights (zeros)
        self.weights = np.zeros(X.shape[1])
        
        # Gradient descent
        for i in range(self.n_iterations):
            # Linear combination of inputs and weights
            z = np.dot(X, self.weights)
            
            # Apply sigmoid to get probabilities
            h = self.__sigmoid(z)
            
            # Compute gradient (X.T * (h - y)) / n_samples
            gradient = np.dot(X.T, (h - y)) / y.size
            
            # Update weights
            self.weights -= self.learning_rate * gradient
            
            # Print training progress if verbose
            if self.verbose and i % 100 == 0:
                loss = self.__loss(h, y)
                print(f'Iteration {i}, Loss: {loss}')
    
    def predict_prob(self, X):
        """
        Predict probability of positive class
        
        Parameters:
        X -- Input feature matrix (n_samples, n_features)
        
        Returns:
        Probability of positive class (n_samples,)
        """
        # Standardize features
        X = self.scaler.transform(X)
        
        # Add intercept term
        if self.fit_intercept:
            X = self.__add_intercept(X)
        
        # Compute probabilities
        return self.__sigmoid(np.dot(X, self.weights))
    
    def predict(self, X, threshold=0.5):
        """
        Predict class labels
        
        Parameters:
        X -- Input feature matrix (n_samples, n_features)
        threshold -- Classification threshold (default: 0.5)
        
        Returns:
        Predicted class labels (n_samples,)
        """
        return self.predict_prob(X) >= threshold
```

### å…­ã€é¢ç»
**Q**ï¼šé€»è¾‘å›å½’å’Œæ„ŸçŸ¥æœºçš„åŒºåˆ«ã€‚
>ç®€å•çš„æ„ŸçŸ¥æœºå…¶å®å’Œé€»è¾‘å›å½’ç±»ä¼¼ï¼Œéƒ½æ˜¯æ•°æ®ä¹˜ä¸Šä¸€ä¸ªå›å½’ç³»æ•°çŸ©é˜µ $w$ å¾—åˆ°ä¸€ä¸ªæ•° $y$ï¼Œä¸è¿‡æ„ŸçŸ¥æœºä¸æ±‚æ¦‚ç‡ï¼Œä¸€èˆ¬ä¼šé€‰å–ä¸€ä¸ªåˆ†ç±»è¾¹ç•Œï¼Œå¯èƒ½ $y>0$ å°±æ˜¯ $A$ ç±»åˆ«ï¼Œ$y<0$ å°±æ˜¯ $B$ ç±»åˆ«ã€‚é€»è¾‘å›å½’çš„æŸå¤±å‡½æ•°ç”±æœ€å¤§ä¼¼ç„¶æ¨å¯¼è€Œæ¥ï¼Œç”¨äº¤å‰ç†µæŸå¤±ï¼ŒåŠ›å›¾ä½¿é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®æ¦‚ç‡åˆ†å¸ƒæ¥è¿‘ã€‚æ„ŸçŸ¥æœºçš„æŸå¤±å‡½æ•°å¯èƒ½æœ‰å¤šç§æ–¹æ³•æ ¸å¿ƒæ˜¯é’ˆå¯¹è¯¯åˆ†ç±»ç‚¹åˆ°è¶…å¹³é¢çš„è·ç¦»æ€»å’Œè¿›è¡Œå»ºæ¨¡ï¼Œå³ä½¿é¢„æµ‹çš„ç»“æœä¸çœŸå®ç»“æœè¯¯å·®æ›´å°ï¼Œæ˜¯å»æ±‚å¾—åˆ†ç±»è¶…å¹³é¢ï¼ˆå‡½æ•°æ‹Ÿåˆï¼‰ã€‚è¿™æ˜¯ä¸¤è€…æœ€æœ€æ ¹æœ¬çš„å·®å¼‚ã€‚

**Q**ï¼šä¸ºä»€ä¹ˆé€»è¾‘å›å½’ç”¨äº¤å‰ç†µæŸå¤±è€Œä¸ç”¨MSEï¼Ÿ
>é€»è¾‘å›å½’é€šè¿‡æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œè€Œå¯¹æ•°ä¼¼ç„¶çš„è´Ÿæ•°å°±æ˜¯äº¤å‰ç†µï¼Œå› æ­¤ä¸¤è€…æœ¬è´¨ç›¸åŒã€‚å¦å¤–äº¤å‰ç†µæ˜¯å‡¸å‡½æ•°ï¼Œæ¢¯åº¦æ›´æ–°æ›´é«˜æ•ˆï¼Œè€ŒMSEåœ¨é€»è¾‘å›å½’ä¸­ä¼šå¯¼è‡´éå‡¸ä¼˜åŒ–å’Œæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

[å›åˆ°ç›®å½•](#ç›®å½•)

---


## å†³ç­–æ ‘

### ä¸€ã€å®šä¹‰ä¸æ ¸å¿ƒæ€æƒ³

å†³ç­–æ ‘æ˜¯ä¸€ç§åŸºäºæ ‘ç»“æ„çš„**ç›‘ç£å­¦ä¹ **ç®—æ³•ï¼Œä¸»è¦ç”¨äºåˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚å®ƒé€šè¿‡ä¸€ç³»åˆ—è§„åˆ™å¯¹æ•°æ®è¿›è¡Œåˆ†å‰²ï¼Œæ„å»ºä¸€ä¸ªæ ‘å½¢æ¨¡å‹ï¼Œå…¶ä¸­æ¯ä¸ªå†…éƒ¨èŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªç‰¹å¾æˆ–å±æ€§ä¸Šçš„åˆ¤æ–­æ¡ä»¶ï¼Œæ¯ä¸ªåˆ†æ”¯ä»£è¡¨åˆ¤æ–­æ¡ä»¶çš„å¯èƒ½ç»“æœï¼Œè€Œæ¯ä¸ªå¶èŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªç±»åˆ«ï¼ˆåˆ†ç±»æ ‘ï¼‰æˆ–ä¸€ä¸ªå…·ä½“å€¼ï¼ˆå›å½’æ ‘ï¼‰ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯é€’å½’åœ°é€‰æ‹©æœ€ä¼˜ç‰¹å¾è¿›è¡Œæ•°æ®åˆ’åˆ†ï¼Œä½¿å¾—åˆ’åˆ†åçš„å­é›†å°½å¯èƒ½"çº¯å‡€"ï¼Œå³åŒä¸€ç±»åˆ«çš„æ ·æœ¬å°½å¯èƒ½é›†ä¸­ã€‚

### äºŒã€ç›®æ ‡
å†³ç­–æ ‘çš„ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªæ³›åŒ–èƒ½åŠ›å¼ºã€è§£é‡Šæ€§å¥½çš„æ¨¡å‹ï¼Œå…·ä½“åŒ…æ‹¬ï¼š
- åˆ†ç±»ä»»åŠ¡ï¼šå¶èŠ‚ç‚¹è¡¨ç¤ºç±»åˆ«æ ‡ç­¾ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–åˆ†ç±»å‡†ç¡®æ€§ã€‚
- å›å½’ä»»åŠ¡ï¼šå¶èŠ‚ç‚¹è¡¨ç¤ºè¿ç»­å€¼ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–é¢„æµ‹è¯¯å·®ï¼ˆå¦‚å‡æ–¹è¯¯å·®ï¼‰ã€‚

æ­¤å¤–ï¼Œå†³ç­–æ ‘è¿½æ±‚ï¼š
- å±€éƒ¨æœ€ä¼˜æ€§ï¼šæ¯æ¬¡åˆ’åˆ†é€‰æ‹©å½“å‰æœ€ä¼˜ç‰¹å¾ï¼Œè€Œéå…¨å±€æœ€ä¼˜ã€‚
- å¯è§£é‡Šæ€§ï¼šé€šè¿‡æ ‘å½¢ç»“æ„ç›´è§‚å±•ç¤ºå†³ç­–é€»è¾‘ï¼ˆç±»ä¼¼"if-then"è§„åˆ™ï¼‰ã€‚

### ä¸‰ã€è¯¦ç»†å†…å®¹
æ¨èçœ‹è¿™ç¯‡[çŸ¥ä¹ä¸“æ ](https://zhuanlan.zhihu.com/p/133838427)å’Œè¿™ç¯‡[CSDN](https://blog.csdn.net/GreenYang5277/article/details/104500739)ã€‚

### å››ã€å®ç°æµç¨‹
1. é€‰æ‹©æœ€ä¼˜ç‰¹å¾è¿›è¡Œæ•°æ®åˆ’åˆ†ï¼Œå¸¸ç”¨å‡†åˆ™æœ‰ï¼šID3ç®—æ³•ï¼ˆä¿¡æ¯å¢ç›Šï¼‰ï¼ŒC4.5ç®—æ³•ï¼ˆä¿¡æ¯å¢ç›Šæ¯”ï¼‰ï¼ŒCARTç®—æ³•ï¼ˆåŸºå°¼æŒ‡æ•°ï¼‰
2. ä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œé€’å½’åœ°å¯¹æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œä»¥ä¸‹å†…å®¹ï¼š1.é€‰æ‹©å½“å‰æœ€ä¼˜ç‰¹å¾ä½œä¸ºåˆ’åˆ†æ ‡å‡†ï¼›2.æ ¹æ®ç‰¹å¾å–å€¼å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè‹¥å¹²å­é›†ï¼›3.ä¸ºæ¯ä¸ªå­é›†åˆ›å»ºå­èŠ‚ç‚¹ã€‚ï¼ˆåœæ­¢æ¡ä»¶ï¼š1.å½“å‰èŠ‚ç‚¹æ‰€æœ‰æ ·æœ¬å±äºåŒä¸€ç±»åˆ«ï¼›2.æ²¡æœ‰å‰©ä½™ç‰¹å¾å¯ç”¨äºåˆ’åˆ†ï¼›3.æ ·æœ¬æ•°é‡å°äºé¢„å®šé˜ˆå€¼ï¼‰
3. å†³ç­–æ ‘å‰ªæï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ï¼šé¢„å‰ªæï¼ˆåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æå‰åœæ­¢æ ‘çš„ç”Ÿé•¿ï¼‰ï¼Œåå‰ªæï¼ˆå…ˆç”Ÿæˆå®Œæ•´æ ‘ï¼Œå†è‡ªåº•å‘ä¸Šå‰ªæ

### äº”ã€ä»£ç å®ç°
```python
import numpy as np
from math import log
from collections import Counter

class DecisionTree:
    """
    A decision tree classifier using ID3 algorithm.
    
    Attributes:
        tree (dict): The constructed decision tree.
        max_depth (int): Maximum depth of the tree.
        min_samples_split (int): Minimum number of samples required to split a node.
    """
    
    def __init__(self, max_depth=None, min_samples_split=2):
        """
        Initialize the decision tree.
        
        Args:
            max_depth (int, optional): Maximum depth of the tree. Defaults to None.
            min_samples_split (int, optional): Minimum samples to split. Defaults to 2.
        """
        self.tree = {}
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
    
    def fit(self, X, y, depth=0):
        """
        Build the decision tree from training data.
        
        Args:
            X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).
            y (numpy.ndarray): Target vector of shape (n_samples,).
            depth (int, optional): Current depth of the tree. Defaults to 0.
            
        Returns:
            dict: The constructed decision tree.
        """
        n_samples, n_features = X.shape
        n_classes = len(np.unique(y))
        
        # Stopping criteria
        if (n_classes == 1 or 
            n_samples < self.min_samples_split or 
            (self.max_depth is not None and depth == self.max_depth)):
            return self._most_common_label(y)
        
        # Find the best split
        best_feature, best_threshold = self._best_split(X, y, n_features)
        
        # If no split improves purity, return leaf node
        if best_feature is None:
            return self._most_common_label(y)
        
        # Split the dataset
        left_indices = X[:, best_feature] <= best_threshold
        right_indices = X[:, best_feature] > best_threshold
        
        # Recursively build left and right subtrees
        left_subtree = self.fit(X[left_indices], y[left_indices], depth+1)
        right_subtree = self.fit(X[right_indices], y[right_indices], depth+1)
        
        # Return the decision node
        return {
            'feature_index': best_feature,
            'threshold': best_threshold,
            'left': left_subtree,
            'right': right_subtree
        }
    
    def _best_split(self, X, y, n_features):
        """
        Find the best feature and threshold to split on.
        
        Args:
            X (numpy.ndarray): Feature matrix.
            y (numpy.ndarray): Target vector.
            n_features (int): Number of features.
            
        Returns:
            tuple: (best_feature_index, best_threshold)
        """
        best_gain = -1
        best_feature, best_threshold = None, None
        
        for feature_idx in range(n_features):
            thresholds = np.unique(X[:, feature_idx])
            for threshold in thresholds:
                gain = self._information_gain(X, y, feature_idx, threshold)
                
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature_idx
                    best_threshold = threshold
        
        return best_feature, best_threshold
    
    def _information_gain(self, X, y, feature_idx, threshold):
        """
        Calculate information gain from splitting on a feature and threshold.
        
        Args:
            X (numpy.ndarray): Feature matrix.
            y (numpy.ndarray): Target vector.
            feature_idx (int): Index of feature to split on.
            threshold (float): Threshold value for the split.
            
        Returns:
            float: Information gain.
        """
        # Parent entropy
        parent_entropy = self._entropy(y)
        
        # Split data
        left_indices = X[:, feature_idx] <= threshold
        right_indices = X[:, feature_idx] > threshold
        
        if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:
            return 0
        
        # Calculate weighted child entropy
        n = len(y)
        n_left, n_right = len(y[left_indices]), len(y[right_indices])
        child_entropy = (n_left/n) * self._entropy(y[left_indices]) + \
                        (n_right/n) * self._entropy(y[right_indices])
        
        # Information gain is difference in entropy
        return parent_entropy - child_entropy
    
    def _entropy(self, y):
        """
        Calculate entropy of a target vector.
        
        Args:
            y (numpy.ndarray): Target vector.
            
        Returns:
            float: Entropy value.
        """
        counts = np.bincount(y)
        probabilities = counts / len(y)
        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])
    
    def _most_common_label(self, y):
        """
        Find the most common label in a target vector.
        
        Args:
            y (numpy.ndarray): Target vector.
            
        Returns:
            int: The most common class label.
        """
        counter = Counter(y)
        return counter.most_common(1)[0][0]
    
    def predict(self, X):
        """
        Predict class labels for samples in X.
        
        Args:
            X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).
            
        Returns:
            numpy.ndarray: Predicted class labels.
        """
        return np.array([self._predict_single(x, self.tree) for x in X])
    
    def _predict_single(self, x, tree):
        """
        Predict class for a single sample by traversing the tree.
        
        Args:
            x (numpy.ndarray): Single sample features.
            tree (dict): Decision tree or subtree.
            
        Returns:
            int: Predicted class label.
        """
        if not isinstance(tree, dict):
            return tree
        
        feature_idx = tree['feature_index']
        threshold = tree['threshold']
        
        if x[feature_idx] <= threshold:
            return self._predict_single(x, tree['left'])
        else:
            return self._predict_single(x, tree['right'])
```

### å…­ã€é¢ç»
**Q**ï¼šç®€å•è§£é‡Šå†³ç­–æ ‘æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ
>å†³ç­–æ ‘åƒä¸€æ£µå€’ç½®çš„æ ‘ï¼Œå…³é”®åœ¨äº"åˆ†å±‚æé—®"å’Œ"é€æ­¥åˆ’åˆ†æ•°æ®"ã€‚ä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œæ ¹æ®æ•°æ®çš„ç‰¹å¾ï¼ˆå¦‚å¹´é¾„ã€æ”¶å…¥ï¼‰ä¸€æ­¥æ­¥æé—®ï¼Œæ¯ä¸ªèŠ‚ç‚¹æ˜¯ä¸€ä¸ªåˆ¤æ–­æ¡ä»¶ï¼Œåˆ†æ”¯æ˜¯å¯èƒ½çš„ç­”æ¡ˆï¼Œæœ€ç»ˆåˆ°è¾¾å¶å­èŠ‚ç‚¹å¾—åˆ°é¢„æµ‹ç»“æœã€‚

**Q**ï¼šä»€ä¹ˆæ˜¯ä¿¡æ¯å¢ç›Šï¼Ÿå¦‚ä½•ç”¨å®ƒé€‰æ‹©åˆ†è£‚ç‰¹å¾ï¼Ÿ
>ä¿¡æ¯å¢ç›Šè¡¡é‡ä¸€ä¸ªç‰¹å¾å¯¹åˆ†ç±»çš„å¸®åŠ©æœ‰å¤šå¤§ã€‚æ¯”å¦‚"å­¦å†"ç‰¹å¾å¦‚æœèƒ½å°†å®¢æˆ·æ˜æ˜¾åˆ†ä¸º"ä¹°/ä¸ä¹°"ä¸¤ç»„ï¼Œå®ƒçš„ä¿¡æ¯å¢ç›Šå°±é«˜ã€‚å†³ç­–æ ‘ä¼šä¼˜å…ˆé€‰æ‹©ä¿¡æ¯å¢ç›Šé«˜çš„ç‰¹å¾åˆ†è£‚ï¼Œå› ä¸ºèƒ½æ›´å¹²å‡€åœ°åˆ’åˆ†æ•°æ®ã€‚

**Q**ï¼šå¦‚ä½•é˜²æ­¢å†³ç­–æ ‘è¿‡æ‹Ÿåˆï¼Ÿ
>ï¼ˆ1ï¼‰å‰ªæï¼šæå‰åœæ­¢æ ‘çš„ç”Ÿé•¿ï¼ˆé™åˆ¶æ·±åº¦ï¼‰æˆ–äº‹åå‰ªæ‰ä¸é‡è¦çš„åˆ†æ”¯
ï¼ˆ2ï¼‰é™åˆ¶å‚æ•°ï¼šæ¯”å¦‚è®¾ç½®å¶èŠ‚ç‚¹çš„æœ€å°æ ·æœ¬æ•°ï¼ˆé¿å…ç”¨æå°‘æ•°æ ·æœ¬åšå†³ç­–ï¼‰

**Q**ï¼šå†³ç­–æ ‘ç›¸æ¯”çº¿æ€§æ¨¡å‹çš„ä¼˜åŠ¿ï¼Ÿ
>ï¼ˆ1ï¼‰å¯è§£é‡Šæ€§å¼ºï¼šè§„åˆ™ç±»ä¼¼äºäººç±»å†³ç­–è¿‡ç¨‹
ï¼ˆ2ï¼‰æ— éœ€å¤æ‚é¢„å¤„ç†ï¼šå¯¹ç¼ºå¤±å€¼ã€éçº¿æ€§å…³ç³»æ›´é²æ£’ã€‚
ï¼ˆ3ï¼‰å¤„ç†æ··åˆç‰¹å¾ï¼šåŒæ—¶å¤„ç†æ•°å­—ã€ç±»åˆ«ç‰¹å¾ã€‚

**Q**ï¼šå†³ç­–æ ‘çš„ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
>ï¼ˆ1ï¼‰å®¹æ˜“è¿‡æ‹Ÿåˆï¼šå¯èƒ½è®°ä½å™ªå£°æ•°æ®
ï¼ˆ2ï¼‰ä¸ç¨³å®šï¼šæ•°æ®å¾®å°å˜åŒ–å¯èƒ½å¯¼è‡´æ ‘ç»“æ„å®Œå…¨ä¸åŒ
ï¼ˆ3ï¼‰ä¸æ“…é•¿å…¨å±€å…³ç³»ï¼šæ¯”å¦‚çº¿æ€§å…³ç³»ï¼Œçº¿æ€§æ¨¡å‹æ›´åŠ ç›´æ¥

[å›åˆ°ç›®å½•](#ç›®å½•)

---


## k-å¹³å‡

### ä¸€ã€å®šä¹‰ä¸æ ¸å¿ƒæ€æƒ³
k-means æ˜¯ä¸€ç§ç»å…¸çš„**æ— ç›‘ç£å­¦ä¹ **èšç±»ç®—æ³•ï¼Œç”¨äºå°†æ•°æ®é›†åˆ’åˆ†ä¸º k ä¸ªäº’ä¸é‡å çš„ç°‡ï¼ˆclustersï¼‰ã€‚å…¶ç›®æ ‡æ˜¯é€šè¿‡è¿­ä»£ä¼˜åŒ–ï¼Œå°†æ•°æ®ç‚¹åˆ†é…åˆ°æœ€è¿‘çš„ç°‡ä¸­å¿ƒï¼ˆè´¨å¿ƒï¼‰ï¼Œä½¿å¾—ç°‡å†…æ•°æ®ç‚¹çš„ç›¸ä¼¼æ€§è¾ƒé«˜ï¼Œè€Œä¸åŒç°‡ä¹‹é—´çš„å·®å¼‚æ€§è¾ƒå¤§ã€‚

### äºŒã€ç›®æ ‡

k-means çš„æ ¸å¿ƒç›®æ ‡æ˜¯å°†æ•°æ®é›†åˆ’åˆ†ä¸º k ä¸ªç°‡ï¼ˆclustersï¼‰ï¼Œä½¿å¾—æ¯ä¸ªæ•°æ®ç‚¹å±äºè·ç¦»æœ€è¿‘çš„ç°‡ä¸­å¿ƒã€‚é€šè¿‡åå¤è°ƒæ•´ç°‡ä¸­å¿ƒçš„ä½ç½®ï¼Œk-means ä¸æ–­ä¼˜åŒ–ç°‡å†…çš„ç´§å¯†åº¦ï¼Œä»è€Œè·å¾—å°½é‡ç´§å‡‘ã€å½¼æ­¤åˆ†ç¦»çš„ç°‡ã€‚è¿™å¯ä»¥ç”¨ç°‡å†…å¹³æ–¹è¯¯å·®ï¼ˆWithin-Cluster Sum of Squares, WCSSï¼‰æ¥åº¦é‡ï¼š

$\text{WCSS} = \sum_{i=1}^{K} \sum_{\mathbf{x} \in C_i} \|\mathbf{x} - \mathbf{\mu}_i\|^2$ 
 
- $K$: é¢„è®¾çš„ç°‡æ•°é‡ã€‚  
- $C_i$: ç¬¬ $i$ ä¸ªç°‡çš„é›†åˆã€‚  
- $\mathbf{\mu}_i$: ç¬¬ $i$ ä¸ªç°‡çš„è´¨å¿ƒï¼ˆå‡å€¼å‘é‡ï¼‰ã€‚  
- $\mathbf{x}$: æ•°æ®ç‚¹ã€‚  

### ä¸‰ã€è¯¦ç»†å†…å®¹
æ¨èçœ‹è¿™ç¯‡[çŸ¥ä¹ä¸“æ ](https://zhuanlan.zhihu.com/p/133838427)ã€‚

### å››ã€å®ç°æµç¨‹
1. åˆå§‹åŒ–é˜¶æ®µï¼šé€‰æ‹©è¦èšç±»çš„æ•°é‡ Kï¼Œå¹¶éšæœºé€‰æ‹© K ä¸ªæ•°æ®ç‚¹ä½œä¸ºåˆå§‹èšç±»ä¸­å¿ƒ(è´¨å¿ƒ)
2. è¿­ä»£é˜¶æ®µï¼šå¯¹äºæ¯ä¸ªæ•°æ®ç‚¹ï¼Œè®¡ç®—å…¶ä¸æ‰€æœ‰è´¨å¿ƒçš„è·ç¦»ï¼Œå°†å…¶åˆ†é…åˆ°è·ç¦»æœ€è¿‘çš„è´¨å¿ƒæ‰€åœ¨çš„ç°‡ï¼Œç„¶åå¯¹äºæ¯ä¸ªç°‡ï¼Œé‡æ–°è®¡ç®—å…¶è´¨å¿ƒ(å³è¯¥ç°‡ä¸­æ‰€æœ‰æ•°æ®ç‚¹çš„å‡å€¼)ï¼Œé‡å¤ä¸Šè¿°æ­¥éª¤ï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶
3. åœæ­¢æ¡ä»¶:è´¨å¿ƒçš„ä½ç½®å˜åŒ–å°äºæŸä¸ªé˜ˆå€¼ã€è¾¾åˆ°é¢„è®¾çš„æœ€å¤§è¿­ä»£æ¬¡æ•°æˆ–ç°‡çš„åˆ†é…ä¸å†æ”¹å˜

### äº”ã€ä»£ç å®ç°
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

class KMeans:
    """
    K-means clustering algorithm implementation
    
    Parameters:
    -----------
    n_clusters: int
        Number of clusters to form
    max_iter: int
        Maximum number of iterations
    tol: float
        Tolerance to declare convergence (if centroids move less than tol)
    random_state: int
        Seed for random initialization
    """
    
    def __init__(self, n_clusters=3, max_iter=100, tol=1e-4, random_state=42):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.tol = tol
        self.random_state = random_state
        self.centroids = None
        self.labels = None
        
    def _initialize_centroids(self, X):
        """Randomly initialize centroids by selecting K points from dataset"""
        np.random.seed(self.random_state)
        random_idx = np.random.permutation(X.shape[0])
        centroids = X[random_idx[:self.n_clusters]]
        return centroids
    
    def _compute_distances(self, X, centroids):
        """
        Compute distances between each data point and each centroid
        
        Parameters:
        -----------
        X: ndarray of shape (n_samples, n_features)
            Input data
        centroids: ndarray of shape (n_clusters, n_features)
            Current centroids
            
        Returns:
        --------
        distances: ndarray of shape (n_samples, n_clusters)
            Distance from each point to each centroid
        """
        distances = np.zeros((X.shape[0], self.n_clusters))
        for k in range(self.n_clusters):
            # Compute Euclidean distance (L2 norm)
            distances[:, k] = np.linalg.norm(X - centroids[k], axis=1)
        return distances
    
    def _assign_clusters(self, distances):
        """
        Assign each data point to the nearest centroid
        
        Parameters:
        -----------
        distances: ndarray of shape (n_samples, n_clusters)
            Distance from each point to each centroid
            
        Returns:
        --------
        labels: ndarray of shape (n_samples,)
            Cluster index for each data point
        """
        return np.argmin(distances, axis=1)
    
    def _update_centroids(self, X, labels):
        """
        Update centroids as the mean of data points in each cluster
        
        Parameters:
        -----------
        X: ndarray of shape (n_samples, n_features)
            Input data
        labels: ndarray of shape (n_samples,)
            Cluster assignments
            
        Returns:
        --------
        centroids: ndarray of shape (n_clusters, n_features)
            New centroids
        """
        centroids = np.zeros((self.n_clusters, X.shape[1]))
        for k in range(self.n_clusters):
            # Compute mean of points assigned to cluster k
            centroids[k] = np.mean(X[labels == k], axis=0)
        return centroids
    
    def fit(self, X):
        """
        Fit K-means algorithm to the input data
        
        Parameters:
        -----------
        X: ndarray of shape (n_samples, n_features)
            Input data
        """
        # Initialize centroids
        self.centroids = self._initialize_centroids(X)
        
        for i in range(self.max_iter):
            # Compute distances from points to centroids
            distances = self._compute_distances(X, self.centroids)
            
            # Assign points to nearest centroid
            self.labels = self._assign_clusters(distances)
            
            # Store current centroids for convergence check
            old_centroids = self.centroids.copy()
            
            # Update centroids
            self.centroids = self._update_centroids(X, self.labels)
            
            # Check for convergence (if centroids don't move much)
            centroid_shift = np.linalg.norm(self.centroids - old_centroids)
            if centroid_shift < self.tol:
                print(f"Converged at iteration {i}")
                break
    
    def predict(self, X):
        """
        Predict cluster labels for new data
        
        Parameters:
        -----------
        X: ndarray of shape (n_samples, n_features)
            New data to predict
            
        Returns:
        --------
        labels: ndarray of shape (n_samples,)
            Predicted cluster labels
        """
        distances = self._compute_distances(X, self.centroids)
        return self._assign_clusters(distances)
```

### å…­ã€é¢ç»
**Q**ï¼šå¦‚ä½•é€‰æ‹©åˆå§‹ä¸­å¿ƒç‚¹ï¼Ÿæœ‰å“ªäº›æ”¹è¿›æ–¹æ³•ï¼Ÿ
>ä¼ ç»Ÿ K-means éšæœºé€‰æ‹©åˆå§‹ä¸­å¿ƒç‚¹å¯èƒ½å¯¼è‡´æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ã€‚æ”¹è¿›æ–¹æ³•åŒ…æ‹¬ k-means++ï¼Œå®ƒé€šè¿‡è®©åˆå§‹ä¸­å¿ƒç‚¹å½¼æ­¤è¿œç¦»æ¥æé«˜æ•ˆæœã€‚å…·ä½“æ˜¯å…ˆéšæœºé€‰ä¸€ä¸ªä¸­å¿ƒï¼Œç„¶åæŒ‰è·ç¦»æ¯”ä¾‹æ¦‚ç‡é€‰æ‹©ä¸‹ä¸€ä¸ªä¸­å¿ƒï¼Œé‡å¤ç›´åˆ°é€‰å¤Ÿ K ä¸ªã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å…ˆç”¨é¢†åŸŸçŸ¥è¯†åˆå§‹åŒ–ä¸­å¿ƒç‚¹ï¼Œæˆ–è€…å¤šæ¬¡è¿è¡Œå–æœ€ä¼˜ç»“æœã€‚

**Q**ï¼šK-means æœ‰å“ªäº›å±€é™æ€§å’Œç¼ºç‚¹ï¼Ÿ
>K-means æœ‰å‡ ä¸ªä¸»è¦é™åˆ¶ï¼š(1) éœ€è¦é¢„å…ˆæŒ‡å®š K å€¼ï¼Œ(2) å¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼Œ(3) å‡è®¾ç°‡æ˜¯çƒå½¢ä¸”å¤§å°ç›¸è¿‘ï¼Œ(4) å¯èƒ½æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è½®å»“ç³»æ•°ç¡®å®š K å€¼ï¼Œé¢„å¤„ç†æ—¶å»é™¤å¼‚å¸¸å€¼ï¼Œæˆ–è€…å°è¯• GMM ç­‰æ›´çµæ´»çš„ç®—æ³•ã€‚ä¸è¿‡å¯¹äºå¤§è§„æ¨¡ã€ç›¸å¯¹å‡åŒ€çš„æ•°æ®é›†ï¼ŒK-means ä»ç„¶æ˜¯é«˜æ•ˆçš„é€‰æ‹©ã€‚

**Q**ï¼šå¦‚ä½•å¤„ç† K-means å¯¹æ•°æ®å°ºåº¦æ•æ„Ÿçš„é—®é¢˜ï¼Ÿ
>ç”±äº K-means ä¾èµ–æ¬§æ°è·ç¦»ï¼Œä¸åŒç‰¹å¾çš„é‡çº²ä¼šå½±å“èšç±»ç»“æœã€‚è§£å†³æ–¹æ³•æ˜¯åœ¨èšç±»å‰è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–ï¼Œå¦‚ Z-score æ ‡å‡†åŒ–æˆ– Min-Max ç¼©æ”¾ã€‚ä¾‹å¦‚åœ¨åŒ…å«æ”¶å…¥å’Œå¹´é¾„çš„æ•°æ®ä¸­ï¼Œå¦‚æœä¸æ ‡å‡†åŒ–ï¼Œæ”¶å…¥çš„å½±å“ä¼šè¿œå¤§äºå¹´é¾„ã€‚æ­¤å¤–ï¼Œå¯¹äºåˆ†ç±»å˜é‡å¯ä»¥è€ƒè™‘ä½¿ç”¨ç‹¬çƒ­ç¼–ç ï¼Œæˆ–è€…é€‰ç”¨é€‚åˆæ··åˆæ•°æ®ç±»å‹çš„ç®—æ³•å¦‚ [k-prototypes](https://blog.csdn.net/pearl8899/article/details/134818856)ï¼ˆæ··åˆä½¿ç”¨æ•°å€¼çš„æ¬§æ°è·ç¦»ä¸ç±»åˆ«çš„[æ±‰æ˜è·ç¦»](https://www.cnblogs.com/BlogNetSpace/p/18221423)ï¼‰ã€‚

**Q**ï¼šå†³ç­–æ ‘å¦‚ä½•åšå›å½’é—®é¢˜ï¼Ÿ
>å†³ç­–æ ‘é€šè¿‡æ„å»ºæ ‘ç»“æ„å°†è¾“å…¥ç©ºé—´åˆ’åˆ†ä¸ºå¤šä¸ªåŒºåŸŸï¼Œå¹¶åœ¨æ¯ä¸ªå¶èŠ‚ç‚¹è¾“å‡ºè¯¥åŒºåŸŸå†…æ ·æœ¬ç›®æ ‡å€¼çš„å‡å€¼ï¼ˆæˆ–ä¸­ä½æ•°ï¼‰ä½œä¸ºè¿ç»­å€¼é¢„æµ‹ã€‚å…¶æ ¸å¿ƒæ˜¯é€šè¿‡é€’å½’é€‰æ‹©ç‰¹å¾å’Œåˆ†è£‚é˜ˆå€¼ï¼Œä»¥æœ€å°åŒ–å­èŠ‚ç‚¹çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æˆ–å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ï¼Œæœ€ç»ˆç”Ÿæˆçš„åˆ†å‰²è§„åˆ™ä½¿åŒåŒºåŸŸå†…çš„æ•°æ®å°½å¯èƒ½ç›¸ä¼¼ï¼Œä½†æœ¬è´¨ä¸Šè¿˜æ˜¯åˆ†ç±»ã€‚

[å›åˆ°ç›®å½•](#ç›®å½•)

---

## éšæœºæ£®æ—

### ä¸€ã€å®šä¹‰ä¸æ ¸å¿ƒæ€æƒ³
ä¸€ç§åŸºäºé›†æˆå­¦ä¹ ï¼ˆEnsemble Learningï¼‰çš„**ç›‘ç£å­¦ä¹ **ç®—æ³•ï¼Œä¸»è¦ç”¨äºåˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚å®ƒé€šè¿‡æ„å»ºå¤šæ£µå†³ç­–æ ‘ï¼ˆDecision Treesï¼‰å¹¶ç»“åˆå®ƒä»¬çš„é¢„æµ‹ç»“æœæ¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚éšæœºæ£®æ—ç”±å¤šæ£µå†³ç­–æ ‘ç»„æˆï¼Œæ¯æ£µæ ‘ç‹¬ç«‹è®­ç»ƒå¹¶æŠ•ç¥¨ï¼ˆåˆ†ç±»ï¼‰æˆ–å¹³å‡ï¼ˆå›å½’ï¼‰ï¼Œç„¶åå¼•å…¥ä¸¤ç§éšæœºæ€§ï¼ˆç‰¹å¾éšæœºé€‰æ‹©ã€æ•°æ®éšæœºé‡‡æ ·ï¼‰æ¥å¢å¼ºå¤šæ ·æ€§ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ "é›†ä½“æ™ºæ…§"ï¼Œå³å¤šä¸ªå¼±æ¨¡å‹ç»„åˆæˆä¸€ä¸ªå¼ºæ¨¡å‹

### äºŒã€ç›®æ ‡

- æœ€å¤§åŒ–åˆ†ç±»å‡†ç¡®æ€§æˆ–å›å½’ç²¾åº¦ï¼šé€šè¿‡å¤šæ£µæ ‘çš„é›†ä½“å†³ç­–é™ä½æ–¹å·®ï¼ˆVarianceï¼‰ï¼Œæå‡æ¨¡å‹ç¨³å®šæ€§ã€‚
- æœ€å°åŒ–è¿‡æ‹Ÿåˆï¼šåˆ©ç”¨éšæœºé‡‡æ ·å’Œç‰¹å¾é€‰æ‹©ï¼Œç¡®ä¿æ¯æ£µæ ‘å·®å¼‚è¾ƒå¤§ï¼Œé¿å…æ¨¡å‹è¿‡äºä¾èµ–è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°ã€‚

### ä¸‰ã€å®ç°æµç¨‹
ï¼ˆ1ï¼‰æ•°æ®å‡†å¤‡ï¼šä»åŸå§‹æ•°æ®é›†ä¸­è¿›è¡Œæœ‰æ”¾å›çš„éšæœºæŠ½æ ·ï¼ˆbootstrapæŠ½æ ·ï¼‰ï¼Œç”Ÿæˆå¤šä¸ªè®­ç»ƒå­é›†ï¼Œå¯¹äºåˆ†ç±»é—®é¢˜ï¼Œé€šå¸¸ä¿æŒå­é›†å¤§å°ä¸åŸå§‹æ•°æ®é›†ç›¸åŒï¼›å¯¹äºå›å½’é—®é¢˜ï¼Œå¯ä»¥ç•¥å°
ï¼ˆ2ï¼‰æ„å»ºå†³ç­–æ ‘ï¼šå¯¹æ¯ä¸ªè®­ç»ƒå­é›†æ„å»ºä¸€æ£µå†³ç­–æ ‘ï¼Œåœ¨æ ‘çš„æ¯ä¸ªèŠ‚ç‚¹åˆ†è£‚æ—¶ï¼Œä»æ‰€æœ‰ç‰¹å¾ä¸­éšæœºé€‰å–ä¸€ä¸ªç‰¹å¾å­é›†ï¼Œä»éšæœºç‰¹å¾å­é›†ä¸­é€‰æ‹©æœ€ä½³åˆ†è£‚ç‚¹
ï¼ˆ3ï¼‰ç»„åˆé¢„æµ‹ï¼šå¯¹äºåˆ†ç±»é—®é¢˜ï¼Œé‡‡ç”¨æŠ•ç¥¨æœºåˆ¶ï¼Œæ¯æ£µæ ‘é¢„æµ‹ç±»åˆ«ï¼Œæœ€ç»ˆé€‰æ‹©å¾—ç¥¨æœ€å¤šçš„ç±»åˆ«ï¼›å¯¹äºå›å½’é—®é¢˜ï¼Œé‡‡ç”¨å¹³å‡æœºåˆ¶ï¼Œå–æ‰€æœ‰æ ‘é¢„æµ‹å€¼çš„å¹³å‡å€¼


### å››ã€è¯¦ç»†å†…å®¹
æ¨èçœ‹è¿™ç¯‡[çŸ¥ä¹ä¸“æ ](https://zhuanlan.zhihu.com/p/471494060)ã€‚


### äº”ã€ä»£ç å®ç°
```python
import numpy as np
from collections import Counter
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

class RandomForest:
    """
    A simple implementation of Random Forest classifier from scratch.
    
    Parameters:
    -----------
    n_estimators: int
        The number of decision trees in the forest.
    max_features: int or float
        The number of features to consider when looking for the best split.
        If int, then consider max_features features at each split.
        If float, then max_features is a fraction and int(max_features * n_features) features are considered.
    max_depth: int
        The maximum depth of the tree.
    min_samples_split: int
        The minimum number of samples required to split an internal node.
    bootstrap: bool
        Whether bootstrap samples are used when building trees.
    random_state: int
        Random seed for reproducibility.
    """
    
    def __init__(self, n_estimators=100, max_features='sqrt', max_depth=None, 
                 min_samples_split=2, bootstrap=True, random_state=None):
        self.n_estimators = n_estimators
        self.max_features = max_features
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.bootstrap = bootstrap
        self.random_state = random_state
        self.trees = []
        self.feature_indices = []  # Stores feature indices used in each tree
        
        if random_state is not None:
            np.random.seed(random_state)
    
    def _bootstrap_sample(self, X, y):
        """
        Create bootstrap sample from the training data.
        
        Parameters:
        -----------
        X: array-like
            Training features.
        y: array-like
            Training labels.
            
        Returns:
        --------
        X_sample: array-like
            Bootstrap sample of features.
        y_sample: array-like
            Bootstrap sample of labels.
        oob_indices: array-like
            Indices of out-of-bag samples.
        """
        n_samples = X.shape[0]
        indices = np.random.choice(n_samples, size=n_samples, replace=True)
        oob_indices = np.array([i for i in range(n_samples) if i not in indices])
        
        X_sample = X[indices]
        y_sample = y[indices]
        
        return X_sample, y_sample, oob_indices
    
    def _get_feature_subset(self, n_features):
        """
        Get random subset of features for a tree.
        
        Parameters:
        -----------
        n_features: int
            Total number of features.
            
        Returns:
        --------
        feature_indices: array-like
            Indices of selected features.
        """
        if isinstance(self.max_features, int):
            m = min(self.max_features, n_features)
        elif isinstance(self.max_features, float):
            m = int(self.max_features * n_features)
        elif self.max_features == 'sqrt':
            m = int(np.sqrt(n_features))
        else:
            m = n_features
            
        return np.random.choice(n_features, m, replace=False)
    
    def fit(self, X, y):
        """
        Build a forest of decision trees from the training set (X, y).
        
        Parameters:
        -----------
        X: array-like
            Training features.
        y: array-like
            Training labels.
        """
        n_samples, n_features = X.shape
        self.classes_ = np.unique(y)
        self.trees = []
        self.feature_indices = []
        
        for _ in range(self.n_estimators):
            # Create bootstrap sample
            if self.bootstrap:
                X_sample, y_sample, _ = self._bootstrap_sample(X, y)
            else:
                X_sample, y_sample = X, y
                
            # Get random feature subset
            feature_idx = self._get_feature_subset(n_features)
            X_sample_subset = X_sample[:, feature_idx]
            
            # Grow decision tree
            tree = DecisionTreeClassifier(
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                max_features='auto'  # Use all features in the subset (already subsetted)
            )
            tree.fit(X_sample_subset, y_sample)
            
            # Save tree and feature indices
            self.trees.append(tree)
            self.feature_indices.append(feature_idx)
    
    def predict(self, X):
        """
        Predict class for X.
        
        Parameters:
        -----------
        X: array-like
            Input features.
            
        Returns:
        --------
        predictions: array-like
            Predicted class labels.
        """
        # Get predictions from all trees
        tree_preds = np.array([tree.predict(X[:, features]) 
                              for tree, features in zip(self.trees, self.feature_indices)])
        
        # Majority voting
        predictions = np.array([Counter(tree_preds[:, i]).most_common(1)[0][0] 
                              for i in range(X.shape[0])])
        return predictions
    
    def predict_proba(self, X):
        """
        Predict class probabilities for X.
        
        Parameters:
        -----------
        X: array-like
            Input features.
            
        Returns:
        --------
        proba: array-like
            Class probabilities of the input samples.
        """
        # Get predictions from all trees
        tree_preds = np.array([tree.predict(X[:, features]) 
                              for tree, features in zip(self.trees, self.feature_indices)])
        
        # Calculate probabilities
        proba = np.zeros((X.shape[0], len(self.classes_)))
        for i in range(X.shape[0]):
            counts = np.bincount(tree_preds[:, i], minlength=len(self.classes_))
            proba[i, :] = counts / self.n_estimators
            
        return proba
```

### å…­ã€é¢ç»
**Q**ï¼šä¸ºä»€ä¹ˆéšæœºæ£®æ—è¦éšæœºæŠ½æ ·æ•°æ®å’Œç‰¹å¾ï¼Ÿ
>è®©æ¯æ£µæ ‘å­¦åˆ°æ•°æ®çš„ä¸åŒå­é›†ï¼Œå¢åŠ å¤šæ ·æ€§ï¼Œé¿å…æ‰€æœ‰æ ‘éƒ½ä¸€æ ·ã€‚åŒæ—¶ï¼Œé˜²æ­¢æŸä¸€å¼ºç‰¹å¾ä¸»å¯¼æ‰€æœ‰æ ‘ï¼ˆæ¯”å¦‚"å¹´é¾„"æ€»è¢«é€‰ä¸ºæ ¹èŠ‚ç‚¹ï¼‰ï¼Œè®©æ¨¡å‹æ¢ç´¢æ›´å¤šç‰¹å¾å…³ç³»ã€‚


[å›åˆ°ç›®å½•](#ç›®å½•)

---


## æœ´ç´ è´å¶æ–¯

### ä¸€ã€å®šä¹‰ä¸æ ¸å¿ƒæ€æƒ³
ä¸€ç§åŸºäºè´å¶æ–¯å®šç†çš„**ç›‘ç£å­¦ä¹ **ç®—æ³•ï¼Œä¸»è¦ç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯è®¡ç®—ç»™å®šç‰¹å¾ä¸‹æ ·æœ¬å±äºæŸç±»çš„åéªŒæ¦‚ç‡ï¼Œå¹¶é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ä½œä¸ºé¢„æµ‹ç»“æœã€‚å…³é”®å‡è®¾ä¸ºæ‰€æœ‰ç‰¹å¾ç›¸äº’ç‹¬ç«‹ï¼Œå³"æœ´ç´ "ä¸€è¯çš„ç”±æ¥ï¼Œå› æ­¤è”åˆæ¦‚ç‡å¯æ‹†åˆ†ä¸ºå•ç‰¹å¾æ¦‚ç‡çš„ä¹˜ç§¯ã€‚

### äºŒã€ç›®æ ‡
æœ´ç´ è´å¶æ–¯ç®—æ³•çš„ä¼˜åŒ–ç›®æ ‡æ˜¯æ‰¾åˆ°ä½¿åéªŒæ¦‚ç‡ $P(y \mid X)$ æœ€å¤§çš„ç±»åˆ« $y$ï¼Œæ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š

$$
\hat{y} = \arg\max_{y} P(y) \prod_{i=1}^n P(x_i \mid y)
$$

- $P(y)$ï¼šç±»åˆ« $y$ çš„å…ˆéªŒæ¦‚ç‡
- $P(x_i \mid y)$ï¼šç‰¹å¾ $x_i$ åœ¨ç±»åˆ« $y$ ä¸‹çš„æ¡ä»¶æ¦‚ç‡
- $\prod_{i=1}^n P(x_i \mid y)$ï¼šåŸºäºç‰¹å¾ç‹¬ç«‹æ€§å‡è®¾çš„è”åˆæ¦‚ç‡
- $\arg\max_{y}$ï¼šé€‰æ‹©ä½¿åéªŒæ¦‚ç‡æœ€å¤§çš„ç±»åˆ«

### ä¸‰ã€è¯¦ç»†å†…å®¹
æ¨èçœ‹è¿™ç¯‡[çŸ¥ä¹ä¸“æ ](https://zhuanlan.zhihu.com/p/656721603)ã€‚

### å››ã€å®ç°æµç¨‹
1. è®­ç»ƒé˜¶æ®µè¾“å…¥å·²æ ‡æ³¨æ ·æœ¬é›†è¾“å‡ºå„ç±»åˆ«å…ˆéªŒå’Œå„ç‰¹å¾æ¡ä»¶æ¦‚ç‡ï¼Œå³ç»Ÿè®¡æ¯ä¸ªç±»åˆ«å‡ºç°çš„é¢‘ç‡ï¼Œä»¥åŠå¯¹æ¯ä¸ªç‰¹å¾åœ¨æ¯ä¸ªç±»åˆ«ä¸‹ç»Ÿè®¡å…¶é¢‘ç‡
2. é¢„æµ‹é˜¶æ®µæ ¹æ®æ ·æœ¬ç‰¹å¾è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ä½œä¸ºé¢„æµ‹ç±»åˆ«

### äº”ã€ä»£ç å®ç°

```python
import numpy as np

class GaussianNaiveBayes:
    """
    A Gaussian Naive Bayes classifier implementation.
    Assumes features follow normal distribution and are conditionally independent given the class.
    """
    
    def fit(self, X, y):
        """
        Train the Gaussian Naive Bayes model with training data.
        
        Parameters:
        X : numpy array, shape (n_samples, n_features)
            Training feature vectors
        y : numpy array, shape (n_samples,)
            Target class labels
        """
        # Get unique class labels from training data
        self.classes = np.unique(y)
        
        # Initialize dictionaries to store:
        self.mean = {}      # Mean of each feature per class
        self.var = {}       # Variance of each feature per class
        self.priors = {}    # Prior probability of each class
        
        # Calculate statistics for each class
        for c in self.classes:
            # Get samples belonging to current class
            X_c = X[y == c]
            
            # Calculate mean and variance of each feature for this class
            self.mean[c] = X_c.mean(axis=0)  # Mean along each feature column
            self.var[c] = X_c.var(axis=0)   # Variance along each feature column
            
            # Calculate prior probability: P(class=c)
            self.priors[c] = X_c.shape[0] / X.shape[0]
    
    def predict(self, X):
        """
        Make predictions for new samples using the trained model.
        
        Parameters:
        X : numpy array, shape (n_samples, n_features)
            Test feature vectors to predict
            
        Returns:
        predictions : list
            Predicted class labels for each test sample
        """
        predictions = []
        
        # Process each test sample individually
        for x in X:
            posteriors = []
            
            # Calculate posterior probability for each class
            for c in self.classes:
                # Calculate likelihood using Gaussian PDF (simplified computation):
                # P(x|class=c) = product of P(x_i|class=c) for all features
                exponent = -((x - self.mean[c]) ** 2) / (2 * self.var[c])
                likelihood = np.exp(exponent) / np.sqrt(2 * np.pi * self.var[c])
                
                # Calculate posterior: P(class=c|x) âˆ P(x|class=c) * P(class=c)
                posterior = np.prod(likelihood) * self.priors[c]
                posteriors.append(posterior)
            
            # Select class with highest posterior probability
            predicted_class = self.classes[np.argmax(posteriors)]
            predictions.append(predicted_class)
            
        return predictions
```

### å…­ã€é¢ç»
**Q**ï¼šä¸ºä»€ä¹ˆå«æœ´ç´ è´å¶æ–¯ï¼Ÿ
> "æœ´ç´ "æŒ‡çš„æ˜¯å®ƒå¯¹ç‰¹å¾åšäº†ä¸€ä¸ªå¼ºå‡è®¾è®¤ä¸ºæ‰€æœ‰ç‰¹å¾ä¹‹é—´å®Œå…¨ç‹¬ç«‹ã€‚ç°å®ä¸­è¿™å¾ˆå°‘æˆç«‹ï¼ˆæ¯”å¦‚"å¤©æ°”"å’Œ"æ¹¿åº¦"å¯èƒ½ç›¸å…³ï¼‰ï¼Œä½†è¿™ä¸ªå‡è®¾ç®€åŒ–äº†è®¡ç®—ï¼Œä½¿å¾—ç®—æ³•é«˜æ•ˆã€‚

**Q**ï¼šæœ´ç´ è´å¶æ–¯ä¼˜ç‚¹ï¼Ÿ
>**è®¡ç®—å¿«**ï¼š1.å‡è®¾æ‰€æœ‰ç‰¹å¾ç›¸äº’ç‹¬ç«‹ï¼Œè®¡ç®—è”åˆæ¦‚ç‡æ—¶åªéœ€ç®€å•ç›¸ä¹˜ï¼Œæ— éœ€è€ƒè™‘ç‰¹å¾é—´çš„å¤æ‚äº¤äº’ 2.è®­ç»ƒæ—¶åªéœ€ç»Ÿè®¡æ¯ä¸ªç‰¹å¾åœ¨å„ç±»åˆ«ä¸‹çš„é¢‘ç‡ï¼Œé¢„æµ‹æ—¶ç›´æ¥æŸ¥è¡¨è®¡ç®—ï¼Œé€Ÿåº¦æå¿«
**å¯¹å°æ•°æ®å‹å¥½**ï¼š1.åªéœ€ä¼°è®¡æ¯ä¸ªç‰¹å¾çš„è¾¹ç¼˜æ¦‚ç‡ï¼Œè€Œéç‰¹å¾é—´çš„è”åˆæ¦‚ç‡ 2.å‚æ•°å°‘ï¼Œä¸å®¹æ˜“è¿‡æ‹Ÿåˆ
**ç®€å•æ˜“å®ç°**ï¼š1.å¤šæ•°å®ç°åªéœ€é€‰æ‹©åˆ†å¸ƒç±»å‹ï¼Œæ— éœ€åƒSVMè°ƒæ ¸å‡½æ•°æˆ–éšæœºæ£®æ—è°ƒæ ‘æ·±åº¦ 2.æ¨¡å‹æœ¬è´¨æ˜¯ä¸€ä¸ªæ¦‚ç‡ç»Ÿè®¡è¡¨ï¼Œè®­ç»ƒè¿‡ç¨‹åªæ˜¯è®¡æ•°æˆ–è®¡ç®—å‡å€¼/æ–¹å·®ï¼Œæ— éœ€æ¢¯åº¦ä¸‹é™ç­‰è¿­ä»£ä¼˜åŒ–
**å¯¹ç¼ºå¤±æ•°æ®ä¸æ•æ„Ÿ**ï¼š1.è®¡ç®—æ¦‚ç‡æ—¶ï¼Œè‹¥æŸç‰¹å¾ç¼ºå¤±ï¼Œç›´æ¥è·³è¿‡è¯¥ç‰¹å¾çš„ä¹˜ç§¯é¡¹ï¼Œä¸å½±å“å…¶ä»–ç‰¹å¾è´¡çŒ® 2.ç»Ÿè®¡ç‰¹å¾æ¦‚ç‡æ—¶ï¼Œç¼ºå¤±å€¼ä¸å‚ä¸è®¡æ•°ã€‚

**Q**ï¼šæœ´ç´ è´å¶æ–¯ç¼ºç‚¹ï¼Ÿ
>**ç‹¬ç«‹æ€§å‡è®¾å¤ªå¼º**ã€**é›¶æ¦‚ç‡é—®é¢˜**ï¼ˆæ¨¡å‹å¯¹æœªè§ç‰¹å¾ç›´æ¥åˆ¤é›¶æ¦‚ç‡ï¼‰ï¼Œä»¥åŠ**å¯¹è¾“å…¥åˆ†å¸ƒæ•æ„Ÿ**ï¼ˆå¦‚æœæ•°æ®ä¸ç¬¦åˆå‡è®¾ï¼Œæ•ˆæœå¯èƒ½å˜å·®ï¼‰ã€‚



[å›åˆ°ç›®å½•](#ç›®å½•)

---


## æœ€å°äºŒä¹˜æ³•
æœ€å°äºŒä¹˜æ³•æ˜¯ä¸€ç§æ•°å­¦ä¼˜åŒ–æŠ€æœ¯ï¼Œå¸¸ç”¨äºæ•°æ®æ‹Ÿåˆé—®é¢˜ï¼Œç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªå‡½æ•°ï¼Œä½¿å…¶å°½å¯èƒ½å‡†ç¡®åœ°é€¼è¿‘ä¸€ç»„è§‚æµ‹æ•°æ®ã€‚

### ä¸€ã€åŸºæœ¬æ€æƒ³

æœ€å°äºŒä¹˜æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š  
**åœ¨ä¸€ç»„ç»™å®šæ•°æ®ç‚¹ä¸‹ï¼Œé€‰å–ä¸€ä¸ªå‡½æ•°ï¼Œä½¿å¾—è¯¥å‡½æ•°é¢„æµ‹å€¼ä¸å®é™…å€¼ä¹‹é—´çš„â€œè¯¯å·®å¹³æ–¹å’Œâ€æœ€å°ã€‚**

å‡è®¾æœ‰ $n$ ä¸ªè§‚æµ‹æ•°æ®ç‚¹ï¼š

$(x_1, y_1),\ (x_2, y_2),\ \dots,\ (x_n, y_n)$

æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä¸€ä¸ªå‡½æ•°ï¼Œæ¯”å¦‚çº¿æ€§å‡½æ•°ï¼š

$y = ax + b$

ä½¿å¾—é¢„æµ‹å€¼ $\hat{y}_i = ax_i + b$ ä¸çœŸå®å€¼ $y_i$ çš„è¯¯å·®æœ€å°ã€‚è¯¯å·®å¹³æ–¹å’Œä¸ºï¼š

$S(a, b) = \sum_{i=1}^n (y_i - (ax_i + b))^2$

ç›®æ ‡æ˜¯æ±‚ï¼š

$\min_{a, b} S(a, b)$

### äºŒã€æ¨å¯¼è¿‡ç¨‹ï¼ˆçº¿æ€§æ‹Ÿåˆï¼‰

å¯¹ $S(a, b)$ å¯¹å‚æ•° $a$ å’Œ $b$ æ±‚åå¯¼ï¼Œå¹¶ä»¤åå¯¼ä¸º 0ï¼Œå¾—åˆ°æ­£è§„æ–¹ç¨‹ï¼š

$\frac{\partial S}{\partial a} = -2 \sum_{i=1}^n x_i(y_i - ax_i - b) = 0$

$\frac{\partial S}{\partial b} = -2 \sum_{i=1}^n (y_i - ax_i - b) = 0$

æ•´ç†å¾—åˆ°ï¼š

$a \sum x_i^2 + b \sum x_i = \sum x_i y_i$

$a \sum x_i + nb = \sum y_i$

è§£è¿™ä¸ªäºŒå…ƒä¸€æ¬¡æ–¹ç¨‹ç»„å³å¯å¾—åˆ°æœ€ä¼˜çš„ $a$ å’Œ $b$ã€‚

### ä¸‰ã€æ‰©å±•å½¢å¼
å¤šå…ƒçº¿æ€§å›å½’

$y = a_1x_1 + a_2x_2 + \dots + a_kx_k + b$

çŸ©é˜µå½¢å¼è¡¨è¾¾ï¼ˆé€‚ç”¨äºç¼–ç¨‹å®ç°ï¼‰

$\text{ç»™å®š } A\in\mathbb{R}^{n\times p},\ b\in\mathbb{R}^{n},\ \text{æœ€å°åŒ– } \|Ax - b\|_2^2$

è§£ä¸ºï¼š

$x = (A^TA)^{-1}A^Tb \quad (\text{å‰æï¼š} A^TA \text{ å¯é€†})$

>æ¨èé˜…è¯»[çŸ¥ä¹ä¸“æ ](https://zhuanlan.zhihu.com/p/38128785)

[å›åˆ°ç›®å½•](#ç›®å½•)

---


>æœ¬æ–‡ä»£ç éƒ¨åˆ†ç”±AIç”Ÿæˆï¼Œæœªç»è¿‡è¯¦ç»†éªŒè¯ï¼Œä½†å°±ç†è§£æ€è·¯æ¥è¯´è¿˜æ˜¯å¤Ÿç”¨çš„